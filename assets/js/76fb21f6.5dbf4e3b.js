"use strict";(self.webpackChunkminjunsz_github_io=self.webpackChunkminjunsz_github_io||[]).push([[8239],{5057:t=>{t.exports=JSON.parse('{"blogPosts":[{"id":"vi","metadata":{"permalink":"/ml/vi","source":"@site/blog/2023-02-11/variational-inference.mdx","title":"[Optimization] Variational Inference","description":"Variational inference in a method to approximate the posterior $p(\\\\mathbf\\\\vert\\\\mathbf{x})$. This is a key technique for Variational AutoEncoder, one of the most famous generative model.","date":"2023-02-11T00:00:00.000Z","formattedDate":"February 11, 2023","tags":[{"label":"variational inference","permalink":"/ml/tags/variational-inference"},{"label":"probability","permalink":"/ml/tags/probability"},{"label":"ML theory","permalink":"/ml/tags/ml-theory"}],"readingTime":6.25,"hasTruncateMarker":true,"authors":[{"name":"Minjun Park","url":"https://github.com/minjunsz","imageURL":"https://github.com/minjunsz.png","key":"minjun"}],"frontMatter":{"slug":"vi","title":"[Optimization] Variational Inference","authors":["minjun"],"tags":["variational inference","probability","ML theory"]},"nextItem":{"title":"[Optimization] MLE & MAP","permalink":"/ml/mle-map"}},"content":"Variational inference in a method to approximate the posterior $p(\\\\mathbf{z}\\\\vert\\\\mathbf{x})$. This is a key technique for Variational AutoEncoder, one of the most famous generative model.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nWhen we are dealing with posteriors, it is likely to be intractable because of its integration for normalizing constant. Historically, people used Monte Carlo(MC) integration to approximate it. However, MC approximation is costly.\\n\\nVariational Inference(VI) was devised to replace complex approximation with optimization problems. The main benefit of VI is that we **don\'t need to know normalizing constant.** This enables us to calculate posterior $p(z \\\\vert x)$ because integration for normalizing constant is one of the most significant problems. Although VI is mainly used to calculate approximate posterior, **it can be used to approximate any distribution whose normalizing constant is unknown**. VI defines a family of distributions, called **variational family**, and optimizes a \\"distribution difference measure\\", called **KL-divergence**.\\n- We can transform the approximation task into optimization task by parametrizing the variational distributions.\\n- We don\'t need normalizing constant because we use KL-divergence\\n\\n:::info\\nVI is used when we have a distribution which we don\'t know completely but only **know up to normalizing constant**. We define a **parametrized distribution** and **optimize KL divergence**.\\n:::\\n\\n## Key Terminologies\\n\\nBefore we delve into the Variational Inference(VI), it would be better to clarify some keywords: *inference, evidence, evaluation, prediction, variational, and learning*.\\n\\n### What is Statistical Inference?\\n\\n**Statistical inference** is\xa0**the process of using a sample to infer the properties of a population**. In [[Probabilistic Model#Latent Variable Model|latent variable model]], we believe that there are some latent variables that control the events. Therefore, inferring underlying properties can be interpreted as updating latent variables($\\\\mathbf{z}$) based on observations($\\\\mathbf{x}$). Bayes\' rule tell us how to update the information on latent variables.\\n$$\\n\\\\begin{align}\\np(\\\\mathbf{z} \\\\vert \\\\mathbf{x}) = \\\\frac{p(\\\\mathbf{x},\\\\mathbf{z})}{\\\\int d \\\\mathbf{z} p(\\\\mathbf{x},\\\\mathbf{z})}\\n\\\\end{align}\\n$$\\nTo **assess the results of modeling and inference**, we would like to know how well a model fits observed data $x$. We can quantify the fitness between a model and observed data via **marginal likelihood** or **evidence** .\\n$$\\n\\\\begin{align}\\np (\\\\mathbf{x}) &= \\\\int p(\\\\mathbf{x},\\\\mathbf{z}) d\\\\mathbf{z}\\n\\\\end{align}\\n$$\\nOnce the model parameter $\\\\theta$ is determined, we can make **predictions** for new data with the **posterior predictive distribution**. This is acquired by replacing the prior on the latent variable $z$ with the posterior.\\n$$\\n\\\\begin{align}\\np(x_{new}) &= \\\\int_{\\\\mathbf{z}} p(x_{new} \\\\vert \\\\mathbf{z}) p(\\\\mathbf{z}) d\\\\mathbf{z}\\\\\\\\\\np(x_{new} \\\\vert \\\\mathbf{x}) &= \\\\int_{\\\\mathbf{z}} p(x_{new} \\\\vert \\\\mathbf{z}) p(\\\\mathbf{z} \\\\vert \\\\mathbf{x}) d\\\\mathbf{z}\\n\\\\end{align}\\n$$\\n\\n### Why Variational?\\n\\nIn functional analysis, *functional* is a general mapping from a space $X$ into a single (real/complex) value. In this definition, the domain $X$ can be a space of functions. If it is the case, the functional is a function that takes another function as input and output a single value.\\n\\nMany problems involve *finding an optimal input function that maximizes/minimizes the functional*. The mathematical techniques developed to solve this type of problem are collectively known as the\xa0_calculus of variations_.\\n\\nVI defines a KL divergence and it is a functional because it takes two functions(probability distributions) as input and returns a single value(difference between them.) Then, VI finds the optimal input function(variational distribution) which minimizes the KL-divergence. Therefore, this process involves kind of calculus of variation. This is why we call it *Variational* inference.\\n\\n### Learning and Parametrized Model\\n\\nWe introduced probabilistic model for ML problems, and [[Probabilistic Model#Parametrized Data Distribution|parametrized the model]] to learn from the data. With the parametrized model, **learning** can be defined as a process to find an optimal model parameter $\\\\theta$ from observed data. This is **optimizing $\\\\theta$ to maximize the evidence**. For the ease of computation, we **maximize log evidence**.\\n$$\\n\\\\theta_{max} = \\\\arg \\\\max_\\\\theta p_\\\\theta(\\\\mathbf{x}) = \\\\arg \\\\max_\\\\theta \\\\log p_\\\\theta(\\\\mathbf{x})\\n$$\\nVI is a general approach to calculate an approximate posterior, which can be applied whether the probabilistic model is parameterized or not. However, it would be better to keep the parametrized notation for further analyses.\\n\\n:::info Model Param. vs Variational Param.\\n\\n - $\\\\theta$ is a **model parameter** which is introduced to define a probabilistic model.\\n - $\\\\phi$ is a **variational parameter** which is introduced to define a variational family for VI.\\n\\n:::\\n\\n## Variational Inference\\n\\nVariational inference is a method to calculate an approximate distribution $q_{\\\\phi}(\\\\mathbf{z})$ of the posterior distribution $p_{\\\\theta}(\\\\mathbf{z} \\\\vert \\\\mathbf{x})$ with fixed $\\\\theta$. Since we want to get a good approximation, we want to minimize the difference(divergence) between $p_{\\\\theta}(\\\\cdot)$ and $q_{\\\\phi}(\\\\cdot)$ by optimizing $\\\\phi$. KL-divergence is a good metric to measure the difference between two probability distributions.\\n\\n**Objective: minimize KL divergence between surrogate distribution $q_{\\\\phi}(\\\\mathbf{z})$ and posterior distribution $p_{\\\\theta}(\\\\mathbf{z}\\\\vert \\\\mathbf{x})$**\\n$$\\n\\\\min_{\\\\phi} KL(q_{\\\\phi}(\\\\mathbf{z}) \\\\| p_{\\\\theta}(\\\\mathbf{z}\\\\vert \\\\mathbf{x}))\\n$$\\n\\n### Evidence Lower BOund (ELBO)\\n\\nWe cannot compute the objective $KL(q_{\\\\phi}(\\\\mathbf{z}) \\\\| p_{\\\\theta}(\\\\mathbf{z}\\\\vert \\\\mathbf{x}))$ because calculating KL divergence requires $p_{\\\\theta}(\\\\mathbf{z}\\\\vert \\\\mathbf{x})$ which is unknown. Let\'s rewrite the KL divergence.\\n\\n$$\\n\\\\begin{align}\\nKL(q_\\\\phi(\\\\mathbf{z}) \\\\| p_\\\\theta(z\\\\vert \\\\mathbf{x}))\\n&= \\\\mathbb{E}_{\\\\mathbf{z} \\\\sim q_\\\\phi(\\\\mathbf{z})} \\\\Big[ \\\\log\\\\frac{q_\\\\phi(\\\\mathbf{z})}{p_\\\\theta(\\\\mathbf{z} \\\\vert \\\\mathbf{x})} \\\\Big] \\\\\\\\\\n&= \\\\mathbb{E}_{\\\\mathbf{z} \\\\sim q_\\\\phi(\\\\mathbf{z})}[\\\\log q_\\\\phi(\\\\mathbf{z})] - \\\\mathbb{E}_{\\\\mathbf{z} \\\\sim q_\\\\phi(\\\\mathbf{z})}[\\\\log p_\\\\theta(\\\\mathbf{z}\\\\vert \\\\mathbf{x})] \\\\\\\\\\n&= \\\\mathbb{E}_{\\\\mathbf{z} \\\\sim q_\\\\phi(\\\\mathbf{z})}[\\\\log q_\\\\phi(\\\\mathbf{z})] - \\\\mathbb{E}_{\\\\mathbf{z} \\\\sim q_\\\\phi(\\\\mathbf{z})}[\\\\log p_\\\\theta(\\\\mathbf{x},\\\\mathbf{z})] + \\\\log p_\\\\theta(\\\\mathbf{x}) \\\\\\\\\\n&= \\\\mathbb{E}_{\\\\mathbf{z} \\\\sim q_\\\\phi(\\\\mathbf{z})}\\\\Big[ \\\\log \\\\frac{q_\\\\phi(\\\\mathbf{z})}{p_\\\\theta(\\\\mathbf{x},\\\\mathbf{z})}\\\\Big] + \\\\log p_\\\\theta(\\\\mathbf{x}) \\\\\\\\\\nKL(q_\\\\phi(\\\\mathbf{z}) \\\\| p_\\\\theta(z\\\\vert \\\\mathbf{x}))&= KL(q_\\\\phi(\\\\mathbf{z}) \\\\| p_\\\\theta(\\\\mathbf{x},\\\\mathbf{z})) + \\\\log p_\\\\theta(\\\\mathbf{x})\\\\\\\\\\n\\n\\\\log p_\\\\theta(\\\\mathbf{x})\\n&= \\\\underbrace{KL(q_\\\\phi(\\\\mathbf{z}) \\\\| p_\\\\theta(\\\\mathbf{z} \\\\vert \\\\mathbf{x}))}_{\\\\ge 0} \\\\underbrace{- KL(q_\\\\phi(\\\\mathbf{z}) \\\\| p_\\\\theta(\\\\mathbf{x},\\\\mathbf{z}))}_{ELBO}\\n\\\\end{align}\\n$$\\nWe can decompose the $KL(q_\\\\phi(\\\\mathbf{z}) \\\\| p_\\\\theta(\\\\mathbf{z}\\\\vert \\\\mathbf{x}))$ into $KL(q_\\\\phi(\\\\mathbf{z}) \\\\| p_\\\\theta(\\\\mathbf{x},\\\\mathbf{z}))$ and likelihood $\\\\log p_\\\\theta(\\\\mathbf{x})$. Let\'s inspect each of the three terms.\\n\\n- $KL(q_\\\\phi(\\\\mathbf{z}) \\\\| p_\\\\theta(\\\\mathbf{z}\\\\vert \\\\mathbf{x}))$\\n\\t- Something that we want to minimize.\\n\\t- intractable because we don\'t know $p_\\\\theta(z\\\\vert \\\\mathbf{x})$.\\n\\t- This term is non-negative by definition of \\"KL-divergence\\"\\n- ELBO\\n\\t- $ELBO = -KL(q_\\\\phi(\\\\mathbf{z}) \\\\| p_\\\\theta(\\\\mathbf{x},\\\\mathbf{z}))=\\\\mathbb{E}_{\\\\mathbf{z}\\\\sim q_\\\\phi(\\\\mathbf{z})}[\\\\log p_\\\\theta(\\\\mathbf{x},\\\\mathbf{z}) - q_\\\\phi(\\\\mathbf{z})]$\\n\\t- This term is tractable.\\n\\t\\t- In latent variable models, we assume that the complete data likelihood $p_\\\\theta(\\\\mathbf{x},\\\\mathbf{z})$ is tractable.\\n\\t\\t- By construction, $q_\\\\phi(\\\\mathbf{z})$ is tractable\\n- $\\\\log p_\\\\theta(\\\\mathbf{x})$\\n\\t- log marginal likelihood, called *log evidence*.\\n\\t- This term does not depend on $q_\\\\phi(\\\\cdot), \\\\phi$\\n\\nFor a fixed model parameter $\\\\theta$, changing variational parameter $\\\\phi$ does not change evidence $\\\\log p_\\\\theta(\\\\mathbf{x})$. Therefore, **maximizing ELBO by optimizing $\\\\phi$ is equivalent to minimizing $KL(q_\\\\phi(\\\\mathbf{z}) \\\\| p_\\\\theta(\\\\mathbf{z}\\\\vert \\\\mathbf{x}))$. This means that $q_\\\\phi(\\\\cdot)$ approximates $p_{\\\\theta}(\\\\mathbf{z}\\\\vert \\\\mathbf{x})$ well.**\\n\\n$$\\n\\\\begin{align}\\n&With\\\\ fixed\\\\ \\\\theta &\\\\\\\\\\n&\\\\arg \\\\max_{\\\\phi} ELBO = \\\\arg \\\\min_{\\\\phi} KL(q_{\\\\phi}(\\\\mathbf{z})\\\\|p_{\\\\theta}(\\\\mathbf{z}\\\\vert \\\\mathbf{x})) &\\n\\\\end{align}\\n$$\\n\\n### ELBO Interpretation\\n\\n$$\\n\\\\begin{align}\\nELBO\\n&= \\\\mathbb{E}_{\\\\mathbf{z}\\\\sim q_\\\\phi(\\\\mathbf{z})}[\\\\log p_\\\\theta(\\\\mathbf{x},\\\\mathbf{z}) - q_\\\\phi(\\\\mathbf{z})] \\\\\\\\\\n&= \\\\mathbb{E}_{\\\\mathbf{z}\\\\sim q_\\\\phi(\\\\mathbf{z})}[\\\\log p_{\\\\theta}(\\\\mathbf{z}) + \\\\log p_{\\\\theta}(\\\\mathbf{x} \\\\vert\\\\mathbf{z}) - q_\\\\phi(\\\\mathbf{z})] \\\\\\\\\\n&= \\\\mathbb{E}_{\\\\mathbf{z}\\\\sim q_\\\\phi(\\\\mathbf{z})}[\\\\log p_{\\\\theta}(\\\\mathbf{x} \\\\vert\\\\mathbf{z})] - KL(q_{\\\\phi}(\\\\mathbf{z}) \\\\| p_{\\\\theta}(\\\\mathbf{z}))\\n\\\\end{align}\\n$$\\nIf we rewrite the ELBO, it consists of two terms\\n- approximate log likelihood\\n  log likelihood = $\\\\log p_{\\\\theta}(\\\\mathbf{x})=\\\\mathbb{E}_{\\\\mathbf{z}}[\\\\log p_\\\\theta(\\\\mathbf{x}\\\\vert \\\\mathbf{z})] \\\\approx \\\\mathbb{E}_{\\\\mathbf{z} \\\\sim q_{\\\\phi}(\\\\mathbf{z})}[\\\\log p_\\\\theta(\\\\mathbf{x}\\\\vert \\\\mathbf{z})]$.\\n- KL divergence between approximate posterior $q_{\\\\phi}(\\\\mathbf{z})$ and prior $p_{\\\\theta}(\\\\mathbf{z})$.\\n\\nTherefore, maximizing ELBO is balancing between\\n- maximizing log likelihood\\n- minimizing the distance to the prior\\n\\nIn a plain text, we want to maximize ELBO up to its upper bound(log likelihood) while keeping the surrogate distribution close to the prior distribution.\\n\\n### Wrapup\\n\\n- VI is an approach to approximate a distribution whose normalizing constant is intractable.\\n- When we are using a latent variable model, we assume that complete likelihood is tractable. The posterior $p_{\\\\theta}(\\\\mathbf{z}\\\\vert \\\\mathbf{x})=\\\\frac{p_{\\\\theta}(\\\\mathbf{x}, \\\\mathbf{z})}{p_{\\\\theta}(\\\\mathbf{x})}$ is intractable because its denominator(normalizing constant) is intractable. Therefore, VI is appropriate for calculating posterior.\\n- To apply VI, we define a set of variational distributions called variational family. This is a parametrized distribution $q_{\\\\phi}(\\\\mathbf{z})$.\\n- By maximizing ELBO, $q_{\\\\phi}(\\\\mathbf{z})$ becomes a good approximation of $p_{\\\\theta}(\\\\mathbf{z}\\\\vert \\\\mathbf{x})$.\\n\\n## Resources\\n\\n- Main resource [pyro_tutorial1](https://pyro.ai/examples/intro_long.html#Background:-inference,-learning-and-evaluation), [pyro_tutorial2](https://pyro.ai/examples/svi_part_i.html#Model-Learning)\\n- [princeton cos597C lecture note](https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf)"},{"id":"mle-map","metadata":{"permalink":"/ml/mle-map","source":"@site/blog/2023-02-09/mle-map.mdx","title":"[Optimization] MLE & MAP","description":"One of the most important tasks in ML is to optimize parameters in a model. There are various approaches and I\'ll explain the most fundamental methods (MLE & MAP) in this post.","date":"2023-02-09T00:00:00.000Z","formattedDate":"February 9, 2023","tags":[{"label":"optimization","permalink":"/ml/tags/optimization"},{"label":"ML theory","permalink":"/ml/tags/ml-theory"}],"readingTime":2.215,"hasTruncateMarker":true,"authors":[{"name":"Minjun Park","url":"https://github.com/minjunsz","imageURL":"https://github.com/minjunsz.png","key":"minjun"}],"frontMatter":{"slug":"mle-map","title":"[Optimization] MLE & MAP","authors":["minjun"],"tags":["optimization","ML theory"]},"prevItem":{"title":"[Optimization] Variational Inference","permalink":"/ml/vi"},"nextItem":{"title":"Statistical Interpretation of Loss Function","permalink":"/ml/ml-interpretation"}},"content":"One of the most important tasks in ML is to optimize parameters in a model. There are various approaches and I\'ll explain the most fundamental methods (MLE & MAP) in this post.\\n\\n\x3c!--truncate--\x3e\\n\\n## context\\n\\nThese concepts works on a [[Probabilistic Model]]. We assume that the data distribution is parametrized by $\\\\theta$ and the likelihood is defined as below:\\n$$\\nP(\\\\mathbf{x}\\\\vert\\\\theta) = \\\\prod_i p(x_i\\\\vert \\\\theta)\\n$$\\n## Optimization Methods\\n\\n### MLE\\n\\nMaximum Likelihood Estimation(MLE) maximizes the likelihood $P(D\\\\vert\\\\theta)$ by optimizing the model parameter $\\\\theta$. The result of MLE is a point estimation of the model parameter.\\n$$\\n\\\\theta_{MLE}=\\\\arg\\\\max_\\\\theta P(\\\\mathbf{x}\\\\vert\\\\theta)=\\\\arg\\\\max_\\\\theta\\\\prod_i p(x_i\\\\vert\\\\theta)\\n$$\\nThis fits the model to the observed data. However, MLE is susceptible to overfitting the model when the data is insufficient. For example, some rare events might not be included in the dataset. MLE assigns zero probability to the unobserved events.\\n\\n### MAP\\n\\nUnlike MLE,  MAP utilizes the *prior knowledge* on the data distribution. The prior knowledge is represented as a distribution on the parameter $\\\\theta$, $P(\\\\theta)$. Given the observations $D$, we can update the knowledge on $\\\\theta$ by applying Bayes\' theorem.\\n$$\\nP(\\\\theta\\\\vert \\\\mathbf{x})=\\\\frac{P(\\\\mathbf{x}\\\\vert\\\\theta)P(\\\\theta)}{P(\\\\mathbf{x})} \\\\propto P(\\\\mathbf{x}\\\\vert\\\\theta)P(\\\\theta)\\n$$\\nWith the updated distribution on $\\\\theta$, the most plausible value is $\\\\arg\\\\max_\\\\theta P(\\\\theta\\\\vert \\\\mathbf{x})$. This formula can be rewritten in terms of likelihood:\\n$$\\n\\\\theta_{MAP}=\\\\arg\\\\max_\\\\theta P(\\\\theta\\\\vert \\\\mathbf{x})=\\\\arg\\\\max_\\\\theta \\\\prod_i p(x_i\\\\vert\\\\theta)p(\\\\theta)\\n$$\\n\\n:::info Relationship between MLE and MAP\\nIf the prior distribution on $\\\\theta$ is an uniform distribution, MAP converges to MLE.\\n:::\\n\\n## MLE with Latent Variable model\\n\\nIn a simple model without latent variables, MLE finds the optimal $\\\\theta$ that maximizes the likelihood $p_\\\\theta(\\\\mathbf{x})$. Now, we introduced [latent variables](../2022-05-02/probabilistic-model.mdx#latent-variable-model) $\\\\mathbf{z}$ to the model. We can still apply MLE by maximizing the likelihood if it is tractable.\\n$$\\n\\\\begin{align}\\n\\\\text{log likelihood} &= \\\\log p_\\\\theta (\\\\mathbf{x})\\\\\\\\\\n&= \\\\log \\\\int p_\\\\theta(\\\\mathbf{x},\\\\mathbf{z}) d\\\\mathbf{z} \\n\\\\end{align}\\n$$\\nHowever, MLE is inapplicable in most cases. There are two main reasons why we can\'t use MLE:\\n1. Even for a fixed $\\\\theta$, the integral over latent variable $z$ is often intractable. (Evaluating likelihood is intractable.)\\n2. Even if we can calculate the log likelihood for all values of $\\\\theta$, maximizing the log likelihood as a function of $\\\\theta$ can be a difficult non-convex optimization problem. (Cannot guarantee to find the global optimum.)\\n\\nEven if we find an optimal model parameter $\\\\theta_{max}$ somehow, we cannot use it for prediction. To construct the posterior predictive distribution $p_{\\\\theta_{max}}(x_{new}\\\\vert \\\\mathbf{x})$, we need the posterior on the latent variable $p_{\\\\theta_{max}}(\\\\mathbf{z} \\\\vert \\\\mathbf{x})$.  However, the denominator of the posterior is the likelihood $p_{\\\\theta_{max}}(\\\\mathbf{x})=\\\\int p_{\\\\theta_{max}}(\\\\mathbf{x},\\\\mathbf{z}) d\\\\mathbf{z}$, which is intractable in general.\\n$$\\n\\\\begin{align}\\np_{\\\\theta_{max}}(\\\\mathbf{z}\\\\vert \\\\mathbf{x}) &= \\\\frac{p_{\\\\theta_{max}}(\\\\mathbf{x},\\\\mathbf{z})}{\\\\int_z p_{\\\\theta_{max}}(\\\\mathbf{x},\\\\mathbf{z})d\\\\mathbf{z}}\\\\\\\\\\np_{\\\\theta_{max}}(x_{new} \\\\vert \\\\mathbf{x})&= \\\\int_{\\\\mathbf{z}}p_{\\\\theta_{max}}(x_{new} \\\\vert \\\\mathbf{z})p_{\\\\theta_{max}}(\\\\mathbf{z} \\\\vert \\\\mathbf{x}) d\\\\mathbf{z}\\n\\\\end{align}\\n$$"},{"id":"ml-interpretation","metadata":{"permalink":"/ml/ml-interpretation","source":"@site/blog/2022-05-02/loss-function.mdx","title":"Statistical Interpretation of Loss Function","description":"Machine learning(ML) defines a loss function and optimizes its model to minimize the loss. Since ML is based on probability theory and statistics, it is reasonable to interpret the loss function from a statistical perspective.","date":"2022-05-02T12:00:00.000Z","formattedDate":"May 2, 2022","tags":[{"label":"loss function","permalink":"/ml/tags/loss-function"},{"label":"probability","permalink":"/ml/tags/probability"},{"label":"ML theory","permalink":"/ml/tags/ml-theory"}],"readingTime":2.61,"hasTruncateMarker":true,"authors":[{"name":"Minjun Park","url":"https://github.com/minjunsz","imageURL":"https://github.com/minjunsz.png","key":"minjun"}],"frontMatter":{"slug":"ml-interpretation","title":"Statistical Interpretation of Loss Function","authors":["minjun"],"tags":["loss function","probability","ML theory"],"date":"2022-05-02T12:00"},"prevItem":{"title":"[Optimization] MLE & MAP","permalink":"/ml/mle-map"},"nextItem":{"title":"Probabilistic Models in Machine Learning","permalink":"/ml/probabilistic-model"}},"content":"Machine learning(ML) defines a loss function and optimizes its model to minimize the loss. Since ML is based on [probability theory and statistics](./probabilistic-model.mdx), it is reasonable to interpret the loss function from a statistical perspective.\\n\\n\x3c!--truncate--\x3e\\n\\n## Model Output as Deterministic Value\\n\\nWe can interpret ML models without statistics. In this case, the output of the model is directly considered as a deterministic solution for the model. In this case, the loss function can have an arbitrary form as far as it satisfies some constraints:\\n\\n- Loss over a dataset is the sum of individual losses of each data. $Loss(\\\\mathbf{x}) = \\\\sum_{i} Loss(x_i)$\\n- It should give a low value if the output is similar to the true label. Although this is not a necessary condition to be a loss function, the loss function is semantically valid when it satisfies this condition.\\n\\n## Model Output as Noise Distribution Parameter\\n\\nIn statistical interpretation, we assume that the observations include some noise. Therefore, the relationship between input $X$ and output $Y$ is not a deterministic function $Y=f(X)$ but a stochastic process $Y\\\\sim Noise(X)$. We can define the noise distribution to have a parametrized form (ex. Gaussian noise). Then, **the output of the model is considered as the parameters in the noise distribution.**\\n$$\\n\\\\begin{align}\\nY &\\\\sim Noise\\\\ Dist.\\\\\\\\\\n&\\\\sim P(Y \\\\vert w=f_{\\\\theta}(X))\\n\\\\end{align}\\n$$\\n\\n:::note\\n - The parameter $\\\\theta$ is the parameter for the ML model e.g. weights in nueral networks.\\n - The parameter $w$ is the parameter for the noise distribution that we defined.\\n:::\\n\\n### Define Loss Function\\n\\nLet\'s consider an i-th sample $(x_i, y_i)$. Given the input $x_i$, the likelihood of this sample is $P(Y=y_i\\\\vert w=f_\\\\theta(x_i))$. For the ease of notations, I\'ll denote it as $p(y_{i}\\\\vert f_{\\\\theta}(x_{i}))$. Thanks to i.i.d. condition on data, total likelihood over the dataset is as follows:\\n\\n$$\\n\\\\begin{align}\\nP(X=x_{i},Y=y_{i} \\\\vert \\\\theta) &= p(y_{i}\\\\vert f_{\\\\theta}(x_{i})) &&\\\\quad \\\\text{(likelihood over a single data)}\\\\\\\\\\nP(D \\\\vert \\\\theta) &= \\\\prod_{i} p(y_{i} \\\\vert f_{\\\\theta}(x_{i})) &&\\\\quad \\\\text{(likelihood over dataset)}\\n\\\\end{align}\\n$$\\n\\nThe goal is to _maximize_ the likelihood. Considering that loss functions must be represented as a sum of terms, and it is preferred to _minimize_ the loss, **we use negative log likelihood(NLL) as the loss function**.\\n\\n$$  \\n\\\\begin{align*} \\\\max_\\\\theta \\\\prod_i p(y_i|f_\\\\theta(x_i)) &\\\\iff \\\\max_\\\\theta \\\\sum_i \\\\log p(y_i|f_\\\\theta(x_i)) \\\\\\\\ &\\\\iff \\\\min_\\\\theta \\\\sum_i -\\\\log p(y_i|f_\\\\theta(x_i)) \\\\end{align*}  \\n$$\\n\\n### MSE with Fixed Variance Gaussian Noise\\n\\nLet\'s assume that the noise distribution is Gaussian with fixed variance. Then, mean($\\\\mu$) is the only parameter that can be changed when input data($X$) varies. We can rewrite the distribution as follows:\\n$$  \\nP(Y\\\\vert f_\\\\theta(X)) = \\\\mathcal{N}\\\\Big(Y\\\\vert \\\\mu(X), \\\\sigma\\\\Big)  \\n$$\\n\\nBy replacing the $\\\\mu(X)$ with the ML model output $f_\\\\theta(X)$, we can derive that minimizing the negative log likelihood is equivalent to minimizing the squared error.\\n\\n$$  \\n\\\\begin{align*} -\\\\log p(y_i|f_\\\\theta(x_i)) &= -\\\\log [\\\\mathcal{N}\\\\Big(y_i\\\\vert f_\\\\theta(x_i), \\\\sigma\\\\Big)] \\\\\\\\ &\\\\propto \\\\big(y_i-f_\\\\theta(x_i)\\\\big)^{2}\\\\\\\\ \\\\therefore loss\\\\ over\\\\ dataset&\\\\propto \\\\sum_i \\\\big(y_i-f_\\\\theta(x_i)\\\\big)^2\\\\\\\\ &\\\\propto \\\\frac{1}{N}\\\\sum_i \\\\big(y_i-f_\\\\theta(x_i)\\\\big)^2 = mean\\\\ squared\\\\ error \\\\end{align*}  \\n$$\\n\\n### Cross Entropy with Multinomial Distribution Noise\\n  \\nIf we assume the noise distribution to be a multinomial distribution, the loss function must be cross entropy.\\n\\n## Reference\\n\\n- [MSE & Guassian Noise](https://towardsdatascience.com/where-does-mean-squared-error-mse-come-from-2002bbbd7806)\\n- [Cross Entropy & Multinomial Distribution](https://towardsdatascience.com/a-quick-guide-to-cross-entropy-loss-function-8f3410ec6ab1)"},{"id":"probabilistic-model","metadata":{"permalink":"/ml/probabilistic-model","source":"@site/blog/2022-05-02/probabilistic-model.mdx","title":"Probabilistic Models in Machine Learning","description":"Machine learning(ML) is an approach to learn some pattern of data, and leverage it to predict properties of unseen data. The same statement is valid for statistics.","date":"2022-05-02T10:00:00.000Z","formattedDate":"May 2, 2022","tags":[{"label":"probability","permalink":"/ml/tags/probability"},{"label":"ML theory","permalink":"/ml/tags/ml-theory"}],"readingTime":3.655,"hasTruncateMarker":true,"authors":[{"name":"Minjun Park","url":"https://github.com/minjunsz","imageURL":"https://github.com/minjunsz.png","key":"minjun"}],"frontMatter":{"slug":"probabilistic-model","title":"Probabilistic Models in Machine Learning","authors":["minjun"],"tags":["probability","ML theory"],"date":"2022-05-02T10:00"},"prevItem":{"title":"Statistical Interpretation of Loss Function","permalink":"/ml/ml-interpretation"}},"content":"Machine learning(ML) is an approach to learn some pattern of data, and leverage it to predict properties of unseen data. The same statement is valid for statistics.\\nThis is because the essence of ML theory comes from statistics. In this post, I will explain how to introduce probabilistic models and statistics to ML problems.\\n\\n\x3c!--truncate--\x3e\\n\\n## Context Modeling\\n\\nTo apply probability theory on real world problems, we have to express all of the problem-related variables into random variables. Observed examples are called a dataset.\\n- When $n$ is the number of samples, dataset $\\\\mathbf{x}$ is\\n\\t- $\\\\mathbf{x}=\\\\{(x_i,y_i): i\\\\in [n]\\\\}$ in a supervised setting\\n\\t- $\\\\mathbf{x}=\\\\{x_i: i\\\\in[n]\\\\}$ in an unsupervised setting\\n\\nFor the ease of notation, I will denote a data point as $x_i$, but it is straight forward to replace $x_i$ with $(x_i,y_i)$ for the supervised setting.\\n\\n## Assumptions\\n\\nWe have to introduce some assumptions to bridge between machine learning tasks and probability theory. These assumptions may vary from model to model, but they are the most common assumptions widely applied.\\n\\n### Data Distribution\\n\\nWe believe that there is a distribution $P(X)$ where the data are drawn. **Estimating this distribution is a main objective for most machine learning problems.** This distribution is known when we are dealing with synthetic data but it\'s unknown for most real-world problems.\\n\\n### Parametrized Data Distribution\\n\\nConsidering the whole search space is intractable. Therefore, we constrain the search space by assuming that the data distribution follows a certain functional form. This functional form is parametrized by model parameters $\\\\theta$. Thanks to this assumption, we can efficiently explore the search space by optimizing $\\\\theta$.\\n\\n$$\\nX\\\\sim P(X) \\\\approx p(x\\\\vert\\\\theta)\\n$$\\nIt is important to note that **estimating $\\\\theta$ is equivalent to estimating the data distribution $P(X)$.** Therefore, ML methods focuses on optimizing the model parameter $\\\\theta$ based on a given dataset $\\\\mathbf{x}$.\\n\\n#### Notations on Model Parameter\\n\\nYou may encounter a few different notations for the model parameter $\\\\theta$.\\n- $p_\\\\theta(\\\\cdot)$\\n\\t- $\\\\theta$ is unlikely to be of interest.\\n\\t- $\\\\theta$ is used to indicate that the probability distribution is parametrized.\\n- $p(\\\\cdot\\\\vert \\\\theta)$\\n\\t- $\\\\theta$ is considered as a random variable.\\n\\t- This notation is used when we applies a Bayesian approach on the parameter $\\\\theta$.\\n- $p(\\\\cdot\\\\vert z;\\\\theta)$\\n\\t- $\\\\theta$ is considered as a fixed value, not having a statistical interpretation.\\n\\t- Conditioned variable $z$ and $\\\\theta$ do not have an intrinsic difference. Both are included in the density function with given/fixed values. However, the author tries to differentiate them because $\\\\theta$ does not have a statistical interpretation in the context.\\n\\n### i.i.d. Sampling\\n\\nThe dataset is assumed to be independent and identically distributed (i.i.d.). Thanks to this assumption, the likelihood of $\\\\mathbf{x}$ becomes a simple product of likelihoods for each data point.\\n$$\\nP(\\\\mathbf{x}\\\\vert\\\\theta)=\\\\prod_i P(X=x_i\\\\vert\\\\theta)=\\\\prod_i p(x_i\\\\vert\\\\theta)\\n$$\\n\\n## Latent Variable Model\\n\\nA probability model might contain unobserved variables. We call them *latent variables*, denoted by $z$, and the probability model with latent variables is called *latent variable model*. The joint distribution can be decomposed as follows:\\n$$\\np_\\\\theta (\\\\mathbf{x},\\\\mathbf{z}) = p_\\\\theta(\\\\mathbf{x}|\\\\mathbf{z}) p_\\\\theta(\\\\mathbf{z})\\n$$\\nTherefore, the model is composed of three variables\\n- observed random variable $x_i$\\n- unboserved (latent) variables $\\\\mathbf{z}=\\\\{z_{i}:i\\\\in [m]\\\\}$\\n- model parameter $\\\\theta$\\n\\n:::info Independence between variables\\nWe **don\'t assume independence** between variables in general. We just believe that there are some underlying latent variables that control the events. Each observation may have its own independent latent variables, or all observations may be affected by shared latent variables.\\n:::\\n\\n### Complete/Incomplete Data\\n\\nThe dataset $\\\\mathbf{x}=\\\\{x_1, x_2, ..., x_n\\\\}$ is called *incomplete dataset* because it partially describes the statistical process of the dataset. On the other hand, $\\\\mathbf{x} \\\\cup \\\\mathbf{z} = \\\\{x_1,x_2,...x_n,z_1,z_2,...,z_m\\\\}$ is called *complete dataset* because it fully describes the statistical process of the dataset.\\n\\n### Why Do We Introduce Latent Variables?\\n\\nIn most cases, marginal likelihood $p_\\\\theta(\\\\mathbf{x})$ is intractable. However, we may but the complete data likelihood $p_\\\\theta(\\\\mathbf{x},\\\\mathbf{z})$ is tractable. This is why we introduce latent variable models for complex problems.\\n\\n## Notations\\n\\nThroughout my posts, I\'ll use the notations below:\\n\\n- $p(\\\\theta)$: prior distribution (on the model paramter $\\\\theta$)\\n- $p(\\\\theta\\\\vert \\\\mathbf{x})$: posterior distribution (on the model parameter $\\\\theta$)\\n- $\\\\mathbf{x}=\\\\{x_1,x_2,...,x_n\\\\}$ means a set of observations\\n- $\\\\mathbf{z}= \\\\{z_1,z_2,...,z_m\\\\}$ means a set of latent variables\\n- $p_{\\\\theta}(\\\\mathbf{x})=p(\\\\mathbf{x}\\\\vert \\\\theta)= \\\\prod_{i}p_{\\\\theta}(x_i)$: marginal likelihood / evidence\\n- $p_{\\\\theta}(\\\\mathbf{x},\\\\mathbf{z})$: joint likelihood / complete data likelihood\\n- $p_{\\\\theta}(\\\\mathbf{z}\\\\vert \\\\mathbf{x})$: posterior distribution on latent variables"}]}')}}]);