"use strict";(self.webpackChunkminjunsz_github_io=self.webpackChunkminjunsz_github_io||[]).push([[718],{5057:t=>{t.exports=JSON.parse('{"blogPosts":[{"id":"ml-interpretation","metadata":{"permalink":"/ml/ml-interpretation","source":"@site/blog/2022-05-02-ml-interpretation.mdx","title":"Statistical Model Interpretation","description":"Machine learning(ML) is an approach to learn some pattern of data, and leverage it to infer unseen data. The same statement is valid for statistics.","date":"2022-05-02T00:00:00.000Z","formattedDate":"May 2, 2022","tags":[{"label":"statistical interpretation","permalink":"/ml/tags/statistical-interpretation"},{"label":"loss function","permalink":"/ml/tags/loss-function"}],"readingTime":3.495,"hasTruncateMarker":true,"authors":[{"name":"Minjun Park","url":"https://github.com/minjunsz","imageURL":"https://github.com/minjunsz.png","key":"minjun"}],"frontMatter":{"slug":"ml-interpretation","title":"Statistical Model Interpretation","authors":["minjun"],"tags":["statistical interpretation","loss function"]}},"content":"Machine learning(ML) is an approach to learn some pattern of data, and leverage it to infer unseen data. The same statement is valid for statistics.\\nThis is because the essence of ML theory comes from statistics. In this post, I will explain how to interpret the output of ML models from a statistical perspective.\\n\\n\x3c!--truncate--\x3e\\n\\n## Deterministic ML Interpretation\\n\\nUse the output of the model as a solution to the problem. Since the model is a deterministic function, it is a point estimate without stochasticity at inference time.\\n\\n| Component    |                        Interpretation                        |\\n| :----------- | :----------------------------------------------------------: |\\n| Model        | deterministic function $f_\\\\theta(\\\\cdot)$ which returns a deterministic output given a input $x$ |\\n| Loss         | difference between the answer $y$ and the output of the model $f_\\\\theta(x)$ |\\n| Objective    | find a model, which captures patterns in the given dataset $D={(x_1, y_1),...,(x_N, y_N)}$ |\\n| Optimization | train to make the output, $f_\\\\theta(x)$, same as the label, $y$. |\\n\\n## Statistical Interpretation\\n\\n### Glossary\\n\\nIn a nutshell, Bayes\' Theorem says $P(A|B)\\\\propto P(B|A) \\\\cdot P(A)$, and this is the basic theorem for statistical machine learning. Before we dive into the ML interpretation, I briefly summarized the terms that I will use below.\\n\\n| Statistics notation | Description | ML notation         |\\n| ------------------- | --------- | ------------------- |\\n| $A$  | Target variables  | model parameter $w$ |\\n| $B$ | variables that we can observe    | dataset $D$ |\\n| prior, $P(A)$ | speculative distribution on $A$ before observations. | $P(w)$  |\\n| posterior, $P(A\\\\vert B)$ | modified distribution on $A$ after the observation $B$. | $P(w\\\\vert D)$  |\\n| likelihood, $P(B\\\\vert A)$ | the probability to observe $B$ given a fixed target variable $A$. | $P(D\\\\vert w)$       |\\n\\n### Assumptions\\n\\nThe goal is to answer **\\"What would be the outcome Y given the input X?\\"**. This is equivalent to estimating the conditional probability distribution $P(Y\\\\vert X)$. Since estimating the true distribution $P(Y\\\\vert X)$ is intractable, we approximate it with a parametrized distribution $P(Y\\\\vert \\\\text{paramter } w)$. The functional form of the parametrized distribution is our design decision.\\n\\nIn statistical interpretation, **we treat the output of the ML model,$f_\\\\theta (X)$, as the parameter $w$ in the approximate distribution.** The resulting distribution $P(Y\\\\vert w=f_\\\\theta (X))$ is considered as the conditional probability distribution $P(Y\\\\vert X)$.\\n:::note\\n\\n- The parameter $\\\\theta$ is the parameter for the ML model e.g. weights in nueral networks.\\n- The parameter $w$ is the parameter for the conditional probability that we defined.\\n\\n:::\\n\\n### Define Loss Function\\n\\nLet\'s consider an i-th sample $(x_i, y_i)$. Given the input $x_i$, the probability distribution on $y$ is $P(Y\\\\vert f_\\\\theta(x_i))$. **The probability to observe the sample $i$ is  $P(Y=y_i\\\\vert f_\\\\theta(x_i))$. This is the probability to observe $y_i$ given a target variable $\\\\theta$, which is likelihood.** To ease the notation, we denote $P(Y=y_i\\\\vert f_\\\\theta(x_i))=p(y_i\\\\vert f_\\\\theta(x_i))$ for the remaining post. Since the samples are in i.i.d. condition and we know the likelihood for a sample, we can calculate the likelihood for the whole data.\\n$$\\nP(D|\\\\theta)=\\\\prod_i P(Y=y_i|X=x_i,\\\\theta)=\\\\prod_i p(y_i|f_\\\\theta (x_i))\\n$$\\nThe goal is to *maximize* the likelihood. Considering loss functions must be represented as a sum of terms, and it is preferred to *minimize* the loss, **we use negative log likelihood(NLL) as the loss function**.\\n$$\\n\\\\begin{align*}\\n\\\\max_\\\\theta \\\\prod_i p(y_i|f_\\\\theta(x_i))\\n&\\\\iff \\\\max_\\\\theta \\\\sum_i \\\\log p(y_i|f_\\\\theta(x_i)) \\\\\\\\\\n&\\\\iff \\\\min_\\\\theta \\\\sum_i -\\\\log p(y_i|f_\\\\theta(x_i))\\n\\\\end{align*}\\n$$\\nNow, let\'s go back to the parametrized distribution we defined. $P(Y|f_\\\\theta(X))$ means that the label $y$ is non-deterministic although the ML model $f_\\\\theta(x)$ is deterministic. If we assume that the label has a Gaussian noise around a deterministic function,  we can rewrite the distribution as below:\\n$$\\nP(Y\\\\vert f_\\\\theta(X)) = \\\\mathcal{N}\\\\Big(Y\\\\vert \\\\mu(X), \\\\sigma\\\\Big)\\n$$\\nSince the variance is fixed, the only parameter required is $\\\\mu$. By replacing the $\\\\mu(X)$ with the ML model output $f_\\\\theta(X)$, we can derive that minimizing the negative log likelihood is equivalent to minimizing the squared error. [reference](https://towardsdatascience.com/where-does-mean-squared-error-mse-come-from-2002bbbd7806)\\n$$\\n\\\\begin{align*}\\n-\\\\log p(y_i|f_\\\\theta(x_i)) &= -\\\\log [\\\\mathcal{N}\\\\Big(y_i\\\\vert f_\\\\theta(x_i), \\\\sigma\\\\Big)] \\\\\\\\\\n&\\\\propto \\\\big(y_i-f_\\\\theta(x_i)\\\\big)^2\\\\\\\\\\n\\\\therefore loss &\\\\propto \\\\sum_i \\\\big(y_i-f_\\\\theta(x_i)\\\\big)^2\\\\\\\\\\n&\\\\propto \\\\frac{1}{N}\\\\sum_i \\\\big(y_i-f_\\\\theta(x_i)\\\\big)^2 = mean\\\\ squared\\\\ error\\n\\\\end{align*}\\n$$\\nIf we assume the noise distribution to be a multinomial distribution, the loss function must be cross entropy. [reference](https://towardsdatascience.com/a-quick-guide-to-cross-entropy-loss-function-8f3410ec6ab1)"}]}')}}]);